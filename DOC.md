# AlexNet 论文实验部分—结构化深度展开

---

## 1. 数据集、划分与评测协议

论文使用 ILSVRC（ImageNet 子集）进行训练与评测：训练约 120 万张，验证 5 万张，测试 15 万张，类别数 1000。报告指标包含 Top-1 与 Top-5 错误率（Top-5 为“正确标签不在前五置信度预测中”的比例）。

> 论文在 ILSVRC-2010 上进行多数实验（其测试集带标签，便于对照），并在 ILSVRC-2012 上也报告结果（该年测试集不公开标签）。

**测试时裁剪策略**：对 256×256 图像抽取 5 个 224×224 patch（四角+中心）及其水平翻转，共 10 个 patch，softmax 预测取平均。若不使用该 10-crop 平均，错误率会更高（Top-1/Top-5：39.0%/18.3%）。&#x20;

---

## 2. 模型与与实验相关的结构要点（与结果直接相关）

* **整体结构**：5 个卷积层 + 3 个全连接层，最后接 1000-way softmax；所有卷积/全连接层后均用 ReLU。
* **两块 GPU 切分与部分连接**：2/4/5 卷积层仅与同一 GPU 上的前一层特征图相连，3 层与前层所有特征图相连；此设计也影响到后续定性分析中的“专门化现象”。&#x20;

> 以上要点会与“局部响应归一化（LRN）”“重叠池化”“数据增强/Dropout”等实验组件产生交互影响，直接体现在误差率变化中。

---

## 3. 减少过拟合的两大手段与量化收益

论文首先指出：网络参数量约 6000 万，若不加以抑制极易过拟合，因此聚焦于两类策略：数据增强与 Dropout。

### 3.1 数据增强（两种）

**A) 随机平移裁剪 + 水平翻转**
将 256×256 图随机裁成 224×224 patch，并加入水平翻转；训练时在线生成（CPU 生产，GPU 训练），几乎无额外存储负担；若不使用该策略，网络明显过拟合，需被迫缩小网络容量。

**B) PCA 颜色扰动（“颜色抖动”）**
在全体训练像素的 RGB 协方差上做 PCA；对每张训练图按主成分方向加上与特征值成比例的随机扰动（高斯 0.1 标准差）；该策略模拟光照/色温变化的不变性，能将 Top-1 进一步降低 **>1%**。

### 3.2 Dropout（用于前两个全连接层）

训练时以 0.5 概率屏蔽每个隐藏神经元输出，相当于共享权重的指数多“子网络”集成；推理时使用全体神经元并将其输出乘 0.5 近似几何平均。Dropout 有效抑制“复杂共适应”，但会**约使收敛迭代数翻倍**。

---

## 4. 结构与训练技巧的 ablation 结论（论文中的量化增益）

### 4.1 局部响应归一化（LRN）

提出沿“通道维的相邻核图”做亮度型归一化（非减均值的对比归一化），促进“侧抑制”与通道间竞争。超参数：k=2, n=5, α=1e-4, β=0.75；应用在第 1、2 卷积层之后。**收益**：Top-1/Top-5 分别下降 **1.4%/1.2%**；在 CIFAR-10 上也验证了 13%→11% 的提升。

### 4.2 重叠池化（Overlapping Pooling）

设置步长 s=2、窗口 z=3（s\<z）以形成重叠池化，较传统非重叠（s=z=2）能带来**Top-1/Top-5 额外下降 0.4%/0.3%**；且“更不易过拟合”。&#x20;

> **综合观感**：AlexNet 的关键实验信号是“增强 + 正则（Dropout/LRN）+ 结构（重叠池化）”叠加后，稳定带来 1%～数% 级别的 Top-k 改善；这些收益来自不同维度（数据、通道竞争、空间汇聚），协同缓解了大模型在百万级样本下仍然显著的过拟合风险。

---

## 5. 训练细节（可复现实验配方）

* **优化器**：SGD，**batch size 128**，**momentum 0.9**，**weight decay 0.0005**（非仅作正则，实测还能**降低训练误差**）。更新公式与符号定义见原文。
* **学习率日程**：全层同学习率，起始 **0.01**，当验证误差不再改善时**手动除以 10**；整个训练期间共降 **3 次**。
* **权重/偏置初始化**：权重 \~ N(0, 0.01)；第二/四/五卷积层与全连接隐藏层的偏置初始化为 **1**（促使 ReLU 早期获得正激活），其他层偏置为 **0**。
* **计算资源与时长**：在 **2× NVIDIA GTX 580 3GB** 上训练 **约 90 个 epoch**，耗时 **5–6 天**。
* **推断**：采用 10-crop 平均（§1 所述）。

---

## 6. 结果对比与要点

### 6.1 ILSVRC-2010（带测试标签）

AlexNet 的 **Top-1/Top-5 测试错误率：37.5% / 17.0%**；同期最佳传统方法分别 **47.1% / 28.2%**（稀疏编码模型 6 个分类器平均），后来最优传统结果 **45.7% / 25.7%**（Fisher Vector 双分类器平均）。

### 6.2 ILSVRC-2012（不带测试标签）

表 2 报告：单模型/多模型与对比系统的验证/测试错误率；AlexNet 多模型集成（7 CNNs\*）在测试集可达 **Top-5=15.3%**，明显优于基于 FVs 的 26.2%（第二名）。带 \* 的模型为先在 ImageNet 2011 Fall 做“预训练”的变体。

> 论文亦在 ImageNet Fall 2009（10,184 类，8.9M 图）上给出结果：通过增加第六个卷积层，Top-1/Top-5 = 67.4%/40.9%，亦优于文献中的 78.1%/60.9%。

---

## 7. 定性评估与表征分析

* **第一层卷积核可视化**：呈现多种方向/频率选择性以及颜色斑块；两块 GPU 学到的核出现\*\*“颜色通道专门化”\*\*差异（GPU1 多为与颜色无关的核，GPU2 多为颜色相关），该现象对随机初始化不敏感、在多次运行中稳定出现。
* **Top-5 示例与歧义**：偏心目标（如角落的螨虫）仍能识别，多数 Top-5 备选语义合理；个别样例存在拍摄主体模糊带来的标签歧义。
* **特征空间近邻**：用倒数第二层（4096-D）激活的欧氏距离做相似图像检索，得到的近邻在像素 L2 上并不相似，但语义一致（如不同姿态的犬/象）；论文建议可通过**将 4096-D 特征自编码压缩为短二进制码**来构建高效图像检索系统。

---

## 8. 重要经验与结论

* **深度确实重要**：去掉任一中间卷积层，Top-1 会**劣化约 2%**。
* **结构与正则化的组合**：LRN（通道竞争）、重叠池化（更强汇聚与轻正则）、大规模数据增强（几何+颜色）与 Dropout（全连接层）共同决定了最终泛化性能的跃迁幅度。  &#x20;
* **训练细节非“可有可无”**：如 **weight decay=0.0005** 不仅是正则，还是**降低训练误差**的关键；学习率按验证集停滞再十倍衰减的启发式对最终收敛至关重要。&#x20;

---

## 9. 可复现实验清单（工程视角）

1. **数据与预处理**：ImageNet 训练尺寸 256×256 → 训练随机 224×224 + flip；测试 10-crop 平均。
2. **增强**：PCA 颜色扰动（α\~N(0,0.1) 乘以特征值），每图一次、直至下次再采样。
3. **正则**：全连接 (fc1, fc2) 用 Dropout p=0.5；卷积层后按文中位置加 LRN（k=2,n=5,α=1e-4,β=0.75）；池化使用 s=2,z=3 的重叠池化。 &#x20;
4. **优化器**：SGD，batch=128，momentum=0.9，weight decay=5e-4；初始 lr=1e-2，验证不下降则 /10，累计 3 次。&#x20;
5. **初始化**：W\~N(0,0.01)；偏置：conv2/4/5 与 fc 隐藏层 =1，其余=0（促 ReLU 早期正向激活）。
6. **计算与时长**：双 GPU 约 90 个 epoch，5–6 天。

---

## 10. 面向“消融/复现实验”的建议模板

* **单因素消融**：

  * 去掉 PCA 颜色扰动 → 观察 Top-1 是否上升约 1%+；
  * 将 s=z=2（非重叠池化） → Top-1/Top-5 应劣化 \~0.4%/0.3%；
  * 去除 LRN → Top-1/Top-5 应劣化 \~1.4%/1.2%；
  * 关闭 Dropout → 明显过拟合、迭代数减少但验证误差升高。

* **多因素组合**：按“增强→结构→正则”的累计曲线记录增益；并在验证集以相同学习率日程保持可比性。

* **报告规范**：统一使用 10-crop 测试或单中心裁剪，两者结果分别给出（方便横向比较与复现校准）。&#x20;

---